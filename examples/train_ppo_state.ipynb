{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e1d93-a302-427a-addb-546bc8b06d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "from flightning import FLIGHTNING_PATH\n",
    "from flightning.algos import ppo\n",
    "from flightning.envs import rollout, HoveringStateEnv\n",
    "from flightning.envs.wrappers import (\n",
    "    MinMaxObservationWrapper,\n",
    "    NormalizeActionWrapper,\n",
    ")\n",
    "from flightning.modules import ActorCriticPPO\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1752b9-68df-4cb9-ba48-278c7ebae7e0",
   "metadata": {},
   "source": [
    "# Training a State-based Quadrotor Policy With PPO\n",
    "\n",
    "## Seed it\n",
    "\n",
    "All JAX computations are deterministic: Given the same inputs, we get the same outputs. Therefore, we need to use random keys whereever we want to sample from a distributution. Here, we need it to initialize the networks and later set the initial random key for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b39025-eba5-4320-a88e-0988e4bc6aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "key = jax.random.key(seed)\n",
    "key_init, key_ppo = jax.random.split(key, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6e6bf9-6109-4190-bf8b-7dbabcb0da4a",
   "metadata": {},
   "source": [
    "## Setup the Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db11bb0-e61f-48ab-a5ac-67785c6304e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drone_path = FLIGHTNING_PATH + \"/objects/quadrotor_files/example_quad.yaml\"\n",
    "dt = 0.02\n",
    "\n",
    "env = HoveringStateEnv(\n",
    "    max_steps_in_episode=3 * int(1 / dt),\n",
    "    dt=dt,\n",
    "    delay=0.03,\n",
    "    velocity_std=0.1,\n",
    "    yaw_scale=1.0,\n",
    "    pitch_roll_scale=0.1,\n",
    "    omega_std=0.1,\n",
    "    drone_path=drone_path,\n",
    "    reward_sharpness=5.0,\n",
    "    action_penalty_weight=0.5,\n",
    ")\n",
    "env = MinMaxObservationWrapper(env)\n",
    "env = NormalizeActionWrapper(env)\n",
    "\n",
    "# get dims\n",
    "action_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0749d4-c1d5-413d-87d1-63908050b17f",
   "metadata": {},
   "source": [
    "## Define the Actor and Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29086034-7e07-4251-9931-3140b242ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = ActorCriticPPO(\n",
    "    [obs_dim, 512, 512, action_dim], initial_log_std=jnp.log(0.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c15893-e977-4550-b3ff-8549937cf21d",
   "metadata": {},
   "source": [
    "Since all objects we are working with are state-less, the parameters are handles separately from the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a9297-de16-42d8-990b-49489ae8a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_params = policy_net.initialize(key_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0cb859-3d13-4339-a7db-d96165ba05c7",
   "metadata": {},
   "source": [
    "## Setup the Optimizer and Train State\n",
    "\n",
    "The optimizer is the method that will update the network parameters. In our case, we use the Adam optimizer. The train state contains information on how to call the network, the current parameters, and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc04129-06ad-4649-93c2-2b749ec8831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = optax.adam(3e-4)\n",
    "train_state = TrainState.create(\n",
    "    apply_fn=policy_net.apply, params=policy_params, tx=tx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a786303f-8dc4-4982-bf50-ae755fded5ff",
   "metadata": {},
   "source": [
    "## Initial Rollout\n",
    "\n",
    "We define a function to do parallel rollouts, a policy function that packages the network structure and the initial parameters, and we rollout the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ede5e4b-46be-44c8-9562-75d818bca73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rollouts(env, policy, num_rollouts, key):\n",
    "    parallel_rollout = jax.vmap(rollout, in_axes=(None, 0, None))\n",
    "    rollout_keys = jax.random.split(key, num_rollouts)\n",
    "    transitions = parallel_rollout(env, rollout_keys, policy)\n",
    "    return transitions\n",
    "\n",
    "\n",
    "def policy(obs, key):\n",
    "    pi = train_state.apply_fn(train_state.params, obs).pi\n",
    "    return pi.sample(seed=key)\n",
    "\n",
    "\n",
    "transitions = get_rollouts(env, policy, 10, jax.random.key(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd904d-05a9-4009-b710-c0cf08af1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_eval.plot_trajectories(transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91233cbd-3640-4384-ae5d-a280386731c9",
   "metadata": {},
   "source": [
    "## Train the Policy Using PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4869273e-ef43-404e-931a-b3f3f66932e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "res_dict = ppo.train(\n",
    "    env,\n",
    "    train_state,\n",
    "    num_epochs=200,\n",
    "    num_steps_per_epoch=env.max_steps_in_episode,\n",
    "    num_envs=1000,\n",
    "    key=key_ppo,\n",
    "    config=ppo.Config(\n",
    "        num_minibatches=20, ent_coef=0.01, gamma=0.99, update_epochs=4\n",
    "    ),\n",
    ")\n",
    "time_train = time.time() - time_start\n",
    "print(f\"Training time: {time_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccfaba4-5634-4a21-b3f4-0e7e7a0f16ad",
   "metadata": {},
   "source": [
    "The `res_dict`contains the history of rewards and the updated training state. Let's have a look at the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed6b9cb-e98b-4dcc-9ee9-d14b489ad9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_returns = res_dict[\"metrics\"][\"returned_episode_returns\"]\n",
    "returned_episode = res_dict[\"metrics\"][\"returned_episode\"]\n",
    "returns = episode_returns * returned_episode\n",
    "num_returned_episodes = returned_episode.sum(axis=(1, 2))\n",
    "mean_returns = returns.sum(axis=(1, 2)) / num_returned_episodes\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(jnp.array(mean_returns))\n",
    "plt.title(\n",
    "    f\"Final Return: {mean_returns[-1]:.2f}, Training Time: {time_train:.2f} s\"\n",
    ")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089a3cfe-1fbb-4bce-9f18-e5799744eb52",
   "metadata": {},
   "source": [
    "## Evaluate the Trained Policy\n",
    "\n",
    "Get the updated Policy Parameters and Create a new Policy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6af6a-5d52-464c-b26e-af97aedf9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_state = res_dict[\"runner_state\"].train_state\n",
    "\n",
    "def policy_trained(obs, key):\n",
    "    pi = new_train_state.apply_fn(new_train_state.params, obs).pi\n",
    "    return pi.mean()  # pi.sample(seed=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709ba37-d429-4593-a49f-a4af926da6ea",
   "metadata": {},
   "source": [
    "Define the evaluation environment and collect the new rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862cf849-9178-4e2e-b799-9c602696fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_eval = HoveringStateEnv(\n",
    "    max_steps_in_episode=10 * int(1 / dt),\n",
    "    dt=dt,\n",
    "    delay=0.03,\n",
    "    velocity_std=0.1,\n",
    "    yaw_scale=1.0,\n",
    "    pitch_roll_scale=0.1,\n",
    "    omega_std=0.1,\n",
    "    drone_path=drone_path,\n",
    ")\n",
    "env_eval = MinMaxObservationWrapper(env_eval)\n",
    "env_eval = NormalizeActionWrapper(env_eval)\n",
    "\n",
    "transitions_eval = get_rollouts(env_eval, policy_trained, 20, jax.random.key(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb72a97-de1b-45b3-b020-f93e427cd17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_eval.plot_trajectories(transitions_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
