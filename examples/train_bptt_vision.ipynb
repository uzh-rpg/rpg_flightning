{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378debe-0491-479c-a81d-ca96f02be697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "from tqdm import tqdm\n",
    "\n",
    "from flightning import FLIGHTNING_PATH\n",
    "from flightning.algos import bptt\n",
    "from flightning.envs import HoveringFeaturesEnv, rollout\n",
    "from flightning.modules import MLP\n",
    "from flightning.utils.math import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4c041-aebe-43a5-9c48-00a37995b335",
   "metadata": {},
   "source": [
    "# Train a Vision-based Policy Using BPTT\n",
    "\n",
    "Here, we show how to use pretraining and backpropagtion through time (BPTT) to train a quadrotor control policy that only observes visual features as pixel coordinates.\n",
    "\n",
    "## Define the Training Environment and Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c0338-3d16-4ab6-857f-1ce247da6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "key = jax.random.key(0)\n",
    "# define the environment\n",
    "drone_path = FLIGHTNING_PATH + \"/objects/quadrotor_files/kolibri.yaml\"\n",
    "dt = 0.02\n",
    "\n",
    "env = HoveringFeaturesEnv(\n",
    "    max_steps_in_episode=3 * int(1 / 0.02),\n",
    "    dt=0.02,\n",
    "    delay=0.05,\n",
    "    velocity_std=2.,\n",
    "    yaw_scale=1.0,\n",
    "    pitch_roll_scale=0.3,\n",
    "    omega_std=2.,\n",
    "    drone_path=drone_path,\n",
    "    reward_sharpness=5.0,\n",
    "    action_penalty_weight=0.5,\n",
    "    num_last_quad_states=5,\n",
    "    skip_frames=1,\n",
    ")\n",
    "\n",
    "# get dims\n",
    "action_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "\n",
    "# define policy\n",
    "policy_net = MLP(\n",
    "    [obs_dim, 1024, 1024, action_dim],\n",
    "    initial_scale=1.0,\n",
    "    action_bias=env.hovering_action,\n",
    ")\n",
    "key, key_ = jax.random.split(key)\n",
    "policy_params = policy_net.initialize(key_)\n",
    "\n",
    "# create fake trainstate\n",
    "tx_idle = optax.adam(0)\n",
    "train_state_collection = TrainState.create(\n",
    "    apply_fn=policy_net.apply, params=policy_params, tx=tx_idle\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ecd9e5-c09c-421c-a445-597ff63cca15",
   "metadata": {},
   "source": [
    "## Pretraining\n",
    "\n",
    "### Define Policy and Rollout Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0fa1c0-c91e-46a4-a827-6edd50ddb242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env, policy, num_rollouts, key):\n",
    "    parallel_rollout = jax.vmap(\n",
    "        partial(rollout, real_step=True, num_steps=1000),\n",
    "        in_axes=(None, 0, None),\n",
    "    )\n",
    "    rollout_keys = jax.random.split(key, num_rollouts)\n",
    "    transitions = parallel_rollout(env, rollout_keys, policy)\n",
    "    return transitions\n",
    "\n",
    "\n",
    "def policy_collection(obs, key):\n",
    "    return train_state_collection.apply_fn(train_state_collection.params, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eff76c1-65a4-4d9f-a705-8bdfa58c07b0",
   "metadata": {},
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6397d-639e-4e5e-a90e-6349f4373576",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_rollout = time.time()\n",
    "transitions = collect_data(env, policy_collection, 100, jax.random.key(3))\n",
    "time_rollout = time.time() - time_rollout\n",
    "print(f\"Rollout time: {time_rollout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2e28f-3ee2-4eba-af9c-e37f287aa9b8",
   "metadata": {},
   "source": [
    "### Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b29dea-4806-4408-a82e-463beb17eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs: observations\n",
    "observations = transitions.obs\n",
    "observations = jnp.reshape(observations, (-1, observations.shape[-1]))\n",
    "\n",
    "# targets: quadrotor state\n",
    "p = transitions.state.quadrotor_state.p\n",
    "# normalize the position\n",
    "p = normalize(p, env.world_box.min, env.world_box.max)\n",
    "R = transitions.state.quadrotor_state.R\n",
    "v = transitions.state.quadrotor_state.v\n",
    "v = normalize(v, env.v_min, env.v_max)\n",
    "# flatten the last axis of R\n",
    "R = jnp.reshape(R, (*R.shape[:-2], -1))\n",
    "# concatenate the states\n",
    "targets = jnp.concatenate([p, R, v], axis=-1)\n",
    "targets = jnp.reshape(targets, (-1, targets.shape[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270fc5e9-37ac-445b-a15a-e75d51983cc9",
   "metadata": {},
   "source": [
    "### Define the Pretraining Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f2584-9013-44df-ab77-5a31111f47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, obs, targets):\n",
    "    def loss_fn(params):\n",
    "        preds = state.apply_fn(params, obs)\n",
    "        loss = jnp.mean(jnp.abs(preds - targets))\n",
    "        return loss\n",
    "\n",
    "    grads = jax.grad(loss_fn)(state.params)  # Compute gradients\n",
    "    new_state = state.apply_gradients(grads=grads)  # Update parameters\n",
    "    return new_state\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_loop(state, observations, targets, epochs=100, batch_size=32):\n",
    "    dataset_size = observations.shape[0]\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Shuffle the data at the start of each epoch\n",
    "        perm = jax.random.permutation(jax.random.PRNGKey(epoch), dataset_size)\n",
    "        obs_shuffled = observations[perm]\n",
    "        targets_shuffled = targets[perm]\n",
    "\n",
    "        # Iterate over the dataset in batches\n",
    "        for i in range(0, dataset_size, batch_size):\n",
    "            batch_obs = obs_shuffled[i : i + batch_size]\n",
    "            batch_targets = targets_shuffled[i : i + batch_size]\n",
    "\n",
    "            # Perform a training step\n",
    "            state = train_step(state, batch_obs, batch_targets)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd2980-0dda-4491-a976-f073d67c2ca3",
   "metadata": {},
   "source": [
    "### Create Predictor MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b95e61-7506-44b9-b81e-8454df2589b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MLP([obs_dim, 1024, 1024, targets.shape[-1]], initial_scale=.1)\n",
    "predictor_params = predictor.initialize(jax.random.PRNGKey(0))\n",
    "\n",
    "tx_predictor = optax.adam(1e-3)\n",
    "train_state_predictor = TrainState.create(\n",
    "    apply_fn=predictor.apply, params=predictor_params, tx=tx_predictor\n",
    ")\n",
    "epochs = 500\n",
    "batch_size = 1024\n",
    "\n",
    "train_state_predictor_new = train_loop(train_state_predictor, observations,\n",
    "                                       targets,\n",
    "                                       epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda42e32-72a9-47cc-bef1-a165e020f43c",
   "metadata": {},
   "source": [
    "### Copy Parameters to Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2635c-3126-4c18-8f13-441602b3f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_params['params']['Dense_0'] = train_state_predictor_new.params['params']['Dense_0']\n",
    "policy_params['params']['Dense_1'] = train_state_predictor_new.params['params']['Dense_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e16ef3-dd14-491c-a85c-60a63f6dbf7b",
   "metadata": {},
   "source": [
    "## Create Policy Optimization Envirionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab070cc4-aef2-4ad7-b90b-41696657af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HoveringFeaturesEnv(\n",
    "    max_steps_in_episode=3 * int(1 / dt),\n",
    "    dt=dt,\n",
    "    delay=0.05,\n",
    "    velocity_std=0.1,\n",
    "    yaw_scale=1.0,\n",
    "    pitch_roll_scale=0.1,\n",
    "    omega_std=0.1,\n",
    "    drone_path=drone_path,\n",
    "    reward_sharpness=5.0,\n",
    "    action_penalty_weight=0.5,\n",
    "    num_last_quad_states=5,\n",
    "    skip_frames=1,\n",
    ")\n",
    "\n",
    "# create trainstate\n",
    "scheduler = optax.cosine_decay_schedule(1e-3, 1000)\n",
    "tx = optax.adam(scheduler)\n",
    "train_state = TrainState.create(\n",
    "    apply_fn=policy_net.apply, params=policy_params, tx=tx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1da4a4-4c20-4862-8741-27e37a9bb948",
   "metadata": {},
   "source": [
    "## Train the Policy Using BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5552cf5-b90e-43f2-93d0-22c4e51ac9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "res_dict = bptt.train(\n",
    "    env,\n",
    "    train_state,\n",
    "    num_epochs=500,\n",
    "    num_steps_per_epoch=env.max_steps_in_episode,\n",
    "    num_envs=1000,\n",
    "    key=jax.random.key(0),\n",
    ")\n",
    "time_train = time.time() - time_start\n",
    "print(f\"Training time: {time_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a0b376-9f02-4ff1-84e8-ace7bdf31044",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = res_dict[\"metrics\"]\n",
    "plt.plot(-losses)\n",
    "plt.title(f\"Final Reward: {losses[-1]}, Training Time: {time_train}\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da1b60-f309-46a6-ac8d-bc8c69d73c44",
   "metadata": {},
   "source": [
    "## Evaluate the Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c9e1c3-9b3a-44d5-a7b6-ff04fdb7b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_state = res_dict[\"runner_state\"].train_state\n",
    "\n",
    "def policy_trained(obs, key):\n",
    "    return train_state.apply_fn(new_train_state.params, obs)\n",
    "\n",
    "env_eval = HoveringFeaturesEnv(\n",
    "    max_steps_in_episode=10 * int(1 / dt),\n",
    "    dt=dt,\n",
    "    delay=0.05,\n",
    "    velocity_std=0.1,\n",
    "    yaw_scale=1.0,\n",
    "    pitch_roll_scale=0.1,\n",
    "    omega_std=0.1,\n",
    "    drone_path=drone_path,\n",
    "    num_last_quad_states=5,\n",
    "    skip_frames=1,\n",
    ")\n",
    "transitions_eval = get_rollouts(env_eval, policy_trained, 20, jax.random.key(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd10788-d1f3-41d6-86cd-169b9fb8e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_eval.plot_trajectories(transitions_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
